# binary-classification-toolkit
binary-classification-toolkit представляет собой набор инструментов для бинарной классификации, включая различные модели машинного обучения и методы оценки. Этот код предназначен для быстрого прототипирования и тестирования различных алгоритмов на данных, представляющих интерес для исследователей и специалистов по данным.

Особенности
Множество Моделей: Включает в себя различные модели машинного обучения, такие как логистическая регрессия, случайный лес, градиентный бустинг и многие другие.
Стратифицированная Кросс-Валидация: Использует стратифицированную кросс-валидацию для обеспечения сбалансированного разбиения данных.
Метрики Оценки: Включает различные метрики оценки, такие как точность, полнота, F1-оценка и ROC-AUC.
Обработка Ошибок: Включает обработку ошибок для устойчивости к возможным проблемам при вычислении метрик.
Установка
Для работы с этим кодом вам потребуются следующие библиотеки:

scikit-learn
pandas
numpy
xgboost
catboost
Вы можете установить их с помощью следующей команды:
pip install scikit-learn pandas numpy xgboost catboost

Использование
Определите Ваши Данные: Замените X и y вашими данными, где X - это признаки, а y - это метки классов.
Выберите Модели: Вы можете добавить или удалить модели из словаря models в соответствии с вашими потребностями.
Запустите Код: Просто запустите код, и он выполнит обучение, предсказание и оценку для каждой модели, выводя результаты и матрицы ошибок.


Пример вывода:

| Model                             | Accuracy | Precision | Recall  | F1       | ROC-AUC  |
|-----------------------------------|----------|-----------|---------|----------|----------|
| Logistic Regression               | 0.809731 | 0.925619  | 0.850383| 0.885439 | 0.699636 |
| SGD Classifier                    | 0.838509 | 0.921970  | 0.890383| 0.904745 | 0.697413 |
| Random Forest Classifier          | 0.884845 | 0.887665  | 0.993333| 0.937441 | 0.585556 |
| Gradient Boosting Classifier      | 0.858716 | 0.902709  | 0.940055| 0.920095 | 0.634472 |
| Support Vector Classifier         | 0.876149 | 0.875126  | 1.000000| 0.933375 | 0.533333 |
| K-Nearest Neighbors               | 0.876149 | 0.882274  | 0.990055| 0.932859 | 0.561694 |
| XGBoost Classifier                | 0.899172 | 0.913608  | 0.976721| 0.943951 | 0.682805 |
| CatBoost Classifier               | 0.884845 | 0.890346  | 0.990000| 0.937336 | 0.595000 |
| AdaBoost Classifier               | 0.905010 | 0.924159  | 0.970164| 0.946433 | 0.723971 |
| Quadratic Discriminant Analysis   | 0.861698 | 0.866697  | 0.993388| 0.925699 | 0.496694 |
| Extra Trees Classifier            | 0.884845 | 0.887432  | 0.993388| 0.937375 | 0.585583 |

Средняя матрица ошибок для Logistic Regression:
[[ 5.2  4. ]
 [ 9.8 50.4]]

==================================================

Средняя матрица ошибок для SGD Classifier:
[[ 4.6  4.6]
 [ 6.6 53.6]]

==================================================

Средняя матрица ошибок для Random Forest Classifier:
[[ 2.6  6.6]
 [ 0.6 59.6]]

==================================================

Средняя матрица ошибок для Gradient Boosting Classifier:
[[ 3.6  5.6]
 [ 3.6 56.6]]

==================================================

Средняя матрица ошибок для Support Vector Classifier:
[[ 3.2  6. ]
 [ 2.2 58. ]]

==================================================

Средняя матрица ошибок для K-Nearest Neighbors:
[[ 1.2  8. ]
 [ 0.6 59.6]]

==================================================

Средняя матрица ошибок для XGBoost Classifier:
[[ 3.8  5.4]
 [ 1.4 58.8]]

==================================================

Средняя матрица ошибок для CatBoost Classifier:
[[ 2.6  6.6]
 [ 0.8 59.4]]

==================================================

Средняя матрица ошибок для AdaBoost Classifier:
[[ 7.4  1.8]
 [19.6 40.6]]

==================================================

Средняя матрица ошибок для Quadratic Discriminant Analysis:
[[ 0.   9.2]
 [ 0.4 59.8]]

==================================================

Средняя матрица ошибок для Extra Trees Classifier:
[[ 2.   7.2]
 [ 0.8 59.4]]

==================================================
